import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
import xgboost as xgb
import lightgbm as lgb

# ë°ì´í„° ë¡œë”©
train_df = pd.read_csv("data/train.csv")
test_df = pd.read_csv("data/test.csv")

# ì „ì²˜ë¦¬
train_df = train_df.dropna(subset=['k']).fillna(0)
test_df = test_df.fillna(0)

# ê³µí†µ ìˆ˜ì¹˜í˜• í”¼ì²˜ ì„ íƒ
numeric_cols = train_df.select_dtypes(include=['int64', 'float64']).columns
exclude_cols = ['k', 'is_strike']
common_cols = [col for col in numeric_cols if col in test_df.columns and col not in exclude_cols]

X = train_df[common_cols]
y = train_df['k']
test_X = test_df[common_cols]

# ë°ì´í„° ë¶„ë¦¬
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# ì‹¤í—˜í•  ëª¨ë¸ ëª©ë¡
models = {
    "RandomForest": RandomForestClassifier(n_estimators=100, random_state=42),
    "LogisticRegression": LogisticRegression(max_iter=1000),
    "GradientBoosting": GradientBoostingClassifier(n_estimators=100),
    "XGBoost": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    "LightGBM": lgb.LGBMClassifier()
}

results = []

# ëª¨ë¸ ë°˜ë³µ í•™ìŠµ
for name, model in models.items():
    print(f"\nğŸŸ¢ Training {name}...")
    model.fit(X_train, y_train)
    val_pred = model.predict_proba(X_val)[:, 1]
    score = roc_auc_score(y_val, val_pred)
    print(f"âœ… {name} ROC AUC: {score:.4f}")
    results.append((name, score))

# ê²°ê³¼ ì¶œë ¥
print("\nğŸ“Š Summary of Results:")
for name, score in sorted(results, key=lambda x: -x[1]):
    print(f"{name:20}: ROC AUC = {score:.4f}")
